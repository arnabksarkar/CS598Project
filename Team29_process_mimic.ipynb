{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df261ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "import concurrent.futures \n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = \"./mimic_database/\"\n",
    "\n",
    "## Utilities ##\n",
    "\n",
    "def map_dict(elem, dictionary):\n",
    "    if elem in dictionary:\n",
    "        return dictionary[elem]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d103b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proper Classes ##       \n",
    "        \n",
    "class ParseItemID(object):\n",
    "\n",
    "    ''' This class builds the dictionaries depending on desired features '''\n",
    " \n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "\n",
    "        self.feature_names = ['RBCs', 'WBCs', 'platelets', 'hemoglobin', 'hemocrit', \n",
    "                              'atypical lymphocytes', 'bands', 'basophils', 'eosinophils', 'neutrophils',\n",
    "                              'lymphocytes', 'monocytes', 'polymorphonuclear leukocytes', \n",
    "                              'temperature (F)', 'heart rate', 'respiratory rate', 'systolic', 'diastolic',\n",
    "                              'pulse oximetry', \n",
    "                              'troponin', 'HDL', 'LDL', 'BUN', 'INR', 'PTT', 'PT', 'triglycerides', 'creatinine',\n",
    "                              'glucose', 'sodium', 'potassium', 'chloride', 'bicarbonate',\n",
    "                              'blood culture', 'urine culture', 'surface culture', 'sputum' + \n",
    "                              ' culture', 'wound culture', 'Inspired O2 Fraction', 'central venous pressure', \n",
    "                              'PEEP Set', 'tidal volume', 'anion gap',\n",
    "                              'daily weight', 'tobacco', 'diabetes', 'history of CV events']\n",
    "\n",
    "        self.features = ['$^RBC(?! waste)', '$.*wbc(?!.*apache)', '$^platelet(?!.*intake)', \n",
    "                         '$^hemoglobin', '$hematocrit(?!.*Apache)', \n",
    "                         'Differential-Atyps', 'Differential-Bands', 'Differential-Basos', 'Differential-Eos',\n",
    "                         'Differential-Neuts', 'Differential-Lymphs', 'Differential-Monos', 'Differential-Polys', \n",
    "                         'temperature f', 'heart rate', 'respiratory rate', 'systolic', 'diastolic', \n",
    "                         'oxymetry(?! )', \n",
    "                         'troponin', 'HDL', 'LDL', '$^bun(?!.*apache)', 'INR', 'PTT',  \n",
    "                         '$^pt\\\\b(?!.*splint)(?!.*exp)(?!.*leak)(?!.*family)(?!.*eval)(?!.*insp)(?!.*soft)',\n",
    "                         'triglyceride', '$.*creatinine(?!.*apache)', \n",
    "                         '(?<!boost )glucose(?!.*apache).*',\n",
    "                       '$^sodium(?!.*apache)(?!.*bicarb)(?!.*phos)(?!.*ace)(?!.*chlo)(?!.*citrate)(?!.*bar)(?!.*PO)',                          '$.*(?<!penicillin G )(?<!urine )potassium(?!.*apache)', \n",
    "                         '^chloride', 'bicarbonate', 'blood culture', 'urine culture', 'surface culture',\n",
    "                         'sputum culture', 'wound culture', 'Inspired O2 Fraction', '$Central Venous Pressure(?! )',\n",
    "                         'PEEP set', 'tidal volume \\(set\\)', 'anion gap', 'daily weight', 'tobacco', 'diabetes',\n",
    "                         'CV - past']                        \n",
    "\n",
    "        self.patterns = []       \n",
    "        for feature in self.features:\n",
    "            if '$' not in feature:\n",
    "                self.patterns.append('.*{0}.*'.format(feature))\n",
    "            elif '$' in feature:\n",
    "                self.patterns.append(feature[1::])\n",
    "\n",
    "        self.d_items = pd.read_csv(ROOT + 'D_ITEMS.csv', usecols=['ITEMID', 'LABEL'])\n",
    "        self.d_items.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "        self.script_features_names = ['epoetin', 'warfarin', 'heparin', 'enoxaparin', 'fondaparinux',\n",
    "                                      'asprin', 'ketorolac', 'acetominophen', \n",
    "                                      'insulin', 'glucagon', \n",
    "                                      'potassium', 'calcium gluconate', \n",
    "                                      'fentanyl', 'magensium sulfate', \n",
    "                                      'D5W', 'dextrose', \n",
    "                                      'ranitidine', 'ondansetron', 'pantoprazole', 'metoclopramide', \n",
    "                                      'lisinopril', 'captopril', 'statin',  \n",
    "                                      'hydralazine', 'diltiazem', \n",
    "                                      'carvedilol', 'metoprolol', 'labetalol', 'atenolol',\n",
    "                                      'amiodarone', 'digoxin(?!.*fab)',\n",
    "                                      'clopidogrel', 'nitroprusside', 'nitroglycerin',\n",
    "                                      'vasopressin', 'hydrochlorothiazide', 'furosemide', \n",
    "                                      'atropine', 'neostigmine',\n",
    "                                      'levothyroxine',\n",
    "                                      'oxycodone', 'hydromorphone', 'fentanyl citrate', \n",
    "                                      'tacrolimus', 'prednisone', \n",
    "                                      'phenylephrine', 'norepinephrine',\n",
    "                                      'haloperidol', 'phenytoin', 'trazodone', 'levetiracetam',\n",
    "                                      'diazepam', 'clonazepam',\n",
    "                                      'propofol', 'zolpidem', 'midazolam', \n",
    "                                      'albuterol', 'ipratropium', \n",
    "                                      'diphenhydramine',  \n",
    "                                      '0.9% Sodium Chloride',\n",
    "                                      'phytonadione', \n",
    "                                      'metronidazole', \n",
    "                                      'cefazolin', 'cefepime', 'vancomycin', 'levofloxacin',\n",
    "                                      'cipfloxacin', 'fluconazole', \n",
    "                                      'meropenem', 'ceftriaxone', 'piperacillin',\n",
    "                                      'ampicillin-sulbactam', 'nafcillin', 'oxacillin',\n",
    "                                      'amoxicillin', 'penicillin', 'SMX-TMP']\n",
    "\n",
    "        self.script_features = ['epoetin', 'warfarin', 'heparin', 'enoxaparin', 'fondaparinux', \n",
    "                                'aspirin', 'keterolac', 'acetaminophen',\n",
    "                                'insulin', 'glucagon',\n",
    "                                'potassium', 'calcium gluconate',\n",
    "                                'fentanyl', 'magnesium sulfate', \n",
    "                                'D5W', 'dextrose',   \n",
    "                                'ranitidine', 'ondansetron', 'pantoprazole', 'metoclopramide', \n",
    "                                'lisinopril', 'captopril', 'statin',  \n",
    "                                'hydralazine', 'diltiazem', \n",
    "                                'carvedilol', 'metoprolol', 'labetalol', 'atenolol',\n",
    "                                'amiodarone', 'digoxin(?!.*fab)',\n",
    "                                'clopidogrel', 'nitroprusside', 'nitroglycerin',\n",
    "                                'vasopressin', 'hydrochlorothiazide', 'furosemide', \n",
    "                                'atropine', 'neostigmine',\n",
    "                                'levothyroxine',\n",
    "                                'oxycodone', 'hydromorphone', 'fentanyl citrate', \n",
    "                                'tacrolimus', 'prednisone', \n",
    "                                'phenylephrine', 'norepinephrine',\n",
    "                                'haloperidol', 'phenytoin', 'trazodone', 'levetiracetam',\n",
    "                                'diazepam', 'clonazepam',\n",
    "                                'propofol', 'zolpidem', 'midazolam', \n",
    "                                'albuterol', '^ipratropium', \n",
    "                                'diphenhydramine(?!.*%)(?!.*cream)(?!.*/)',  \n",
    "                                '^0.9% sodium chloride(?! )',\n",
    "                                'phytonadione', \n",
    "                                'metronidazole(?!.*%)(?! desensit)', \n",
    "                                'cefazolin(?! )', 'cefepime(?! )', 'vancomycin', 'levofloxacin',\n",
    "                                'cipfloxacin(?!.*ophth)', 'fluconazole(?! desensit)', \n",
    "                                'meropenem(?! )', 'ceftriaxone(?! desensit)', 'piperacillin',\n",
    "                                'ampicillin-sulbactam', 'nafcillin', 'oxacillin', 'amoxicillin',\n",
    "                                'penicillin(?!.*Desen)', 'sulfamethoxazole']\n",
    "\n",
    "        self.script_patterns = ['.*' + feature + '.*' for feature in self.script_features]\n",
    "\n",
    "    def prescriptions_init(self):\n",
    "        self.prescriptions = pd.read_csv(ROOT + 'PRESCRIPTIONS.csv',\n",
    "                                         usecols=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'DRUG',\n",
    "                                                  'STARTDATE', 'ENDDATE'])\n",
    "        self.prescriptions.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "    def query_prescriptions(self, feature_name):\n",
    "        pattern = '.*{0}.*'.format(feature_name)\n",
    "        condition = self.prescriptions['DRUG'].str.contains(pattern, flags=re.IGNORECASE)\n",
    "        return self.prescriptions['DRUG'].where(condition).dropna().values\n",
    "\n",
    "    def extractor(self, feature_name, pattern):\n",
    "        condition = self.d_items['LABEL'].str.contains(pattern, flags=re.IGNORECASE)\n",
    "        dictionary_value = self.d_items['ITEMID'].where(condition).dropna().values.astype('int')\n",
    "        self.dictionary[feature_name] = set(dictionary_value)\n",
    "\n",
    "    def query(self, feature_name):\n",
    "        pattern = '.*{0}.*'.format(feature_name)\n",
    "        print(pattern)\n",
    "        condition = self.d_items['LABEL'].str.contains(pattern, flags=re.IGNORECASE)\n",
    "        return self.d_items['LABEL'].where(condition).dropna().values\n",
    "\n",
    "    def query_pattern(self, pattern):\n",
    "        condition = self.d_items['LABEL'].str.contains(pattern, flags=re.IGNORECASE)\n",
    "        return self.d_items['LABEL'].where(condition).dropna().values\n",
    "\n",
    "    def build_dictionary(self): \n",
    "        assert len(self.feature_names) == len(self.features)\n",
    "        for feature, pattern in zip(self.feature_names, self.patterns):\n",
    "            self.extractor(feature, pattern)\n",
    "\n",
    "    def reverse_dictionary(self, dictionary):\n",
    "        self.rev = {}\n",
    "        for key, value in dictionary.items():\n",
    "            for elem in value:\n",
    "                self.rev[elem] = key\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c4375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimicParser(object):\n",
    "   \n",
    "    ''' This class structures the MIMIC III and builds features then makes 24 hour windows '''\n",
    " \n",
    "    def __init__(self):\n",
    "        self.name = 'mimic_assembler'\n",
    "        self.pid = ParseItemID()\n",
    "        self.pid.build_dictionary()\n",
    "        self.features = self.pid.features\n",
    "\n",
    "    def reduce_total(self, filepath):\n",
    "       \n",
    "        ''' This will filter out rows from CHARTEVENTS.csv that are not feauture relevant '''\n",
    " \n",
    "        #CHARTEVENTS = 330712484 \n",
    "\n",
    "        pid = ParseItemID()\n",
    "        pid.build_dictionary()\n",
    "        chunksize = 10000000\n",
    "        columns = ['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'ITEMID', 'CHARTTIME', 'VALUE',\n",
    "                   'VALUENUM']\n",
    "\n",
    "        for i, df_chunk in enumerate(pd.read_csv(filepath, iterator=True, chunksize=chunksize)):\n",
    "            function = lambda x,y: x.union(y)\n",
    "            df = df_chunk[df_chunk['ITEMID'].isin(reduce(function, pid.dictionary.values()))]\n",
    "            df.dropna(inplace=True, axis=0, subset=columns)\n",
    "            if i == 0:\n",
    "                df.to_csv(ROOT + './mapped_elements/CHARTEVENTS_reduced.csv', index=False, \n",
    "                          columns=columns)\n",
    "                print(i)\n",
    "            else:\n",
    "                df.to_csv(ROOT + './mapped_elements/CHARTEVENTS_reduced.csv', index=False,\n",
    "                          columns=columns, header=None, mode='a')\n",
    "                print(i)\n",
    "            \n",
    "    def map_files(self, shard_number, filename, low_memory=False):\n",
    "        \n",
    "        ''' HADM minimum is 100001 and maximum is 199999. Shards are built off of those. \n",
    "            See if can update based on removing rows from previous buckets to accelerate \n",
    "            speed (each iteration 10% faster) This may not be necessary of reduce total \n",
    "            works well (there are few features)  '''\n",
    "\n",
    "        buckets = []\n",
    "        beg = 100001\n",
    "        end = 199999\n",
    "        interval = math.ceil((end - beg)/float(shard_number))\n",
    "       \n",
    "        for i in np.arange(shard_number):\n",
    "            buckets.append(set(np.arange(beg+(i*interval),beg+(interval+(interval*i)))))\n",
    "\n",
    "        if low_memory==False:\n",
    "            \n",
    "    \n",
    "            for i in range(len(buckets)):\n",
    "                for i,chunk in enumerate(pd.read_csv(filename, iterator=True,\n",
    "                                                     chunksize=10000000)):\n",
    "                    print(buckets[i])\n",
    "                    print(chunk['HADM_ID'].isin(buckets[i]))\n",
    "                    sliced = chunk[chunk['HADM_ID'].astype('int').isin(buckets[i])] \n",
    "                    sliced.to_csv(ROOT + 'mapped_elements/shard_{0}.csv'.format(i), index=False)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            for i in range(len(buckets)):\n",
    "                with open(filename, 'r') as chartevents:\n",
    "                    chartevents.seek(0)\n",
    "                    csvreader = csv.reader(chartevents)\n",
    "                    with open(ROOT+'mapped_elements/shard_{0}.csv'.format(i), 'w') as shard_writer:\n",
    "                        csvwriter = csv.writer(shard_writer)\n",
    "                        for row in csvreader:\n",
    "                            try:\n",
    "                                if row[1] == \"HADM_ID\" or int(row[1]) in buckets[i]:\n",
    "                                    csvwriter.writerow(row)\n",
    "                            except ValueError as e:\n",
    "                                print(row)\n",
    "                                print(e)\n",
    "                         \n",
    "    def create_day_blocks(self, file_name):\n",
    "\n",
    "        ''' Uses pandas to take shards and build them out '''\n",
    "\n",
    "        pid = ParseItemID()\n",
    "        pid.build_dictionary()\n",
    "        pid.reverse_dictionary(pid.dictionary)\n",
    "        df = pd.read_csv(file_name)\n",
    "        df['CHARTDAY'] = df['CHARTTIME'].astype('str').str.split(' ').apply(lambda x: x[0])\n",
    "        df['HADMID_DAY'] = df['HADM_ID'].astype('str') + '_' + df['CHARTDAY']\n",
    "        df['FEATURES'] = df['ITEMID'].apply(lambda x: pid.rev[x])\n",
    "        self.hadm_dict = dict(zip(df['HADMID_DAY'], df['SUBJECT_ID']))\n",
    "        df2 = pd.pivot_table(df, index='HADMID_DAY', columns='FEATURES',\n",
    "                             values='VALUENUM', fill_value=np.nan)\n",
    "        #Team29: lose two columns with low counts (diabetes, blood culture) with above so try this\n",
    "        #        Interestingly this only happened on our Windows setup, not Mac\n",
    "        #https://stackoverflow.com/questions/60647377/why-np-std-and-pivot-tableaggfunc-np-std-return-the-different-result\n",
    "        #df3 = pd.pivot_table(df, index='HADMID_DAY', columns='FEATURES',\n",
    "        #                     values='VALUENUM', aggfunc=np.std, fill_value=0)\n",
    "        df3 = pd.pivot_table(df, index='HADMID_DAY', columns='FEATURES',\n",
    "                             values='VALUENUM', aggfunc=(lambda x: np.std(x)), fill_value=0)\n",
    "        df3.columns = [\"{0}_std\".format(i) for i in list(df2.columns)]\n",
    "        df4 = pd.pivot_table(df, index='HADMID_DAY', columns='FEATURES',\n",
    "                             values='VALUENUM', aggfunc=np.amin, fill_value=np.nan)\n",
    "        df4.columns = [\"{0}_min\".format(i) for i in list(df2.columns)]\n",
    "        df5 = pd.pivot_table(df, index='HADMID_DAY', columns='FEATURES',\n",
    "                             values='VALUENUM', aggfunc=np.amax, fill_value=np.nan)\n",
    "        df5.columns = [\"{0}_max\".format(i) for i in list(df2.columns)]\n",
    "        df2 = pd.concat([df2, df3, df4, df5], axis=1)\n",
    "        df2['tobacco'].apply(lambda x: np.around(x))\n",
    "        del df2['daily weight_std']\n",
    "        del df2['daily weight_min']\n",
    "        del df2['daily weight_max']\n",
    "        del df2['tobacco_std']\n",
    "        del df2['tobacco_min']\n",
    "        del df2['tobacco_max']\n",
    "\n",
    "        rel_columns = list(df2.columns)\n",
    "\n",
    "        rel_columns = [i for i in rel_columns if '_' not in i]\n",
    "\n",
    "        for col in rel_columns:\n",
    "            if len(np.unique(df2[col])[np.isfinite(np.unique(df2[col]))]) <= 2:\n",
    "                print(col)\n",
    "                del df2[col + '_std']\n",
    "                del df2[col + '_min']\n",
    "                del df2[col + '_max']\n",
    "\n",
    "        for i in list(df2.columns):\n",
    "            df2[i][df2[i] > df2[i].quantile(.95)] = df2[i].median()\n",
    "#            if i != 'troponin':\n",
    "#                df2[i] = df2[i].where(df2[i] > df2[i].quantile(.875)).fillna(df2[i].median())\n",
    "\n",
    "        for i in list(df2.columns):\n",
    "            df2[i].fillna(df2[i].median(), inplace=True)\n",
    "            \n",
    "        df2['HADMID_DAY'] = df2.index\n",
    "        df2['INR'] = df2['INR'] + df2['PT']  \n",
    "        df2['INR_std'] = df2['INR_std'] + df2['PT_std']  \n",
    "        df2['INR_min'] = df2['INR_min'] + df2['PT_min']  \n",
    "        df2['INR_max'] = df2['INR_max'] + df2['PT_max']  \n",
    "        del df2['PT']\n",
    "        del df2['PT_std']\n",
    "        del df2['PT_min']\n",
    "        del df2['PT_max']\n",
    "        df2.dropna(thresh=int(0.75*len(df2.columns)), axis=0, inplace=True)\n",
    "        df2.to_csv(file_name[0:-4] + '_24_hour_blocks.csv', index=False)\n",
    "\n",
    "    def add_admissions_columns(self, file_name):\n",
    "        \n",
    "        ''' Add demographic columns to create_day_blocks '''\n",
    "\n",
    "        df = pd.read_csv('./mimic_database/ADMISSIONS.csv')\n",
    "        ethn_dict = dict(zip(df['HADM_ID'], df['ETHNICITY']))\n",
    "        admittime_dict = dict(zip(df['HADM_ID'], df['ADMITTIME']))\n",
    "        df_shard = pd.read_csv(file_name)\n",
    "        df_shard['HADM_ID'] = df_shard['HADMID_DAY'].str.split('_').apply(lambda x: x[0])\n",
    "        df_shard['HADM_ID'] = df_shard['HADM_ID'].astype('int')\n",
    "        df_shard['ETHNICITY'] = df_shard['HADM_ID'].apply(lambda x: map_dict(x, ethn_dict))\n",
    "        black_condition = df_shard['ETHNICITY'].str.contains('.*black.*', flags=re.IGNORECASE)\n",
    "        df_shard['BLACK'] = 0\n",
    "        df_shard['BLACK'][black_condition] = 1\n",
    "        del df_shard['ETHNICITY'] \n",
    "        df_shard['ADMITTIME'] = df_shard['HADM_ID'].apply(lambda x: map_dict(x, admittime_dict))\n",
    "        df_shard.to_csv(file_name[0:-4] + '_plus_admissions.csv', index=False)\n",
    "                \n",
    "    def add_patient_columns(self, file_name):\n",
    "        \n",
    "        ''' Add demographic columns to create_day_blocks '''\n",
    "\n",
    "        df = pd.read_csv('./mimic_database/PATIENTS.csv')\n",
    "\n",
    "        dob_dict = dict(zip(df['SUBJECT_ID'], df['DOB']))\n",
    "        gender_dict = dict(zip(df['SUBJECT_ID'], df['GENDER']))\n",
    "        df_shard = pd.read_csv(file_name)\n",
    "        df_shard['SUBJECT_ID'] = df_shard['HADMID_DAY'].apply(lambda x:\n",
    "                                                               map_dict(x, self.hadm_dict))\n",
    "        df_shard['DOB'] = df_shard['SUBJECT_ID'].apply(lambda x: map_dict(x, dob_dict))\n",
    "\n",
    "        df_shard['YOB'] = df_shard['DOB'].str.split('-').apply(lambda x: x[0]).astype('int')\n",
    "        df_shard['ADMITYEAR'] = df_shard['ADMITTIME'].str.split('-').apply(lambda x: x[0]).astype('int') \n",
    "        \n",
    "        df_shard['AGE'] = df_shard['ADMITYEAR'].subtract(df_shard['YOB']) \n",
    "        df_shard['GENDER'] = df_shard['SUBJECT_ID'].apply(lambda x: map_dict(x, gender_dict))\n",
    "        gender_dummied = pd.get_dummies(df_shard['GENDER'], drop_first=True)\n",
    "        gender_dummied.rename(columns={'M': 'Male', 'F': 'Female'})\n",
    "        COLUMNS = list(df_shard.columns)\n",
    "        COLUMNS.remove('GENDER')\n",
    "        df_shard = pd.concat([df_shard[COLUMNS], gender_dummied], axis=1)\n",
    "        df_shard.to_csv(file_name[0:-4] + '_plus_patients.csv', index=False)\n",
    "\n",
    "    def clean_prescriptions(self, file_name):\n",
    "        \n",
    "        ''' Add prescriptions '''\n",
    "\n",
    "        pid = ParseItemID()\n",
    "        pid.prescriptions_init()        \n",
    "        pid.prescriptions.drop_duplicates(inplace=True)\n",
    "        pid.prescriptions['DRUG_FEATURE'] = np.nan\n",
    "\n",
    "        df_file = pd.read_csv(file_name)\n",
    "        hadm_id_array = pd.unique(df_file['HADM_ID'])\n",
    "\n",
    "        for feature, pattern in zip(pid.script_features_names, pid.script_patterns):\n",
    "           condition = pid.prescriptions['DRUG'].str.contains(pattern, flags=re.IGNORECASE)\n",
    "           pid.prescriptions['DRUG_FEATURE'][condition] = feature\n",
    "\n",
    "        pid.prescriptions.dropna(how='any', axis=0, inplace=True, subset=['DRUG_FEATURE']) \n",
    "\n",
    "        pid.prescriptions.to_csv('./mimic_database/PRESCRIPTIONS_reduced.csv', index=False)\n",
    "        \n",
    "    def add_prescriptions(self, file_name):\n",
    "       \n",
    "        df_file = pd.read_csv(file_name)\n",
    "\n",
    "        with open('./mimic_database/PRESCRIPTIONS_reduced.csv', 'r') as f:\n",
    "            csvreader = csv.reader(f)\n",
    "            with open('./mimic_database/PRESCRIPTIONS_reduced_byday.csv', 'w') as g:\n",
    "                csvwriter  = csv.writer(g)\n",
    "                first_line = csvreader.__next__()\n",
    "                print(first_line[0:3] + ['CHARTDAY'] + [first_line[6]])\n",
    "                csvwriter.writerow(first_line[0:3] + ['CHARTDAY'] + [first_line[6]])\n",
    "                for row in csvreader:\n",
    "                    for i in pd.date_range(row[3], row[4]).strftime('%Y-%m-%d'):\n",
    "                        csvwriter.writerow(row[0:3] + [i] + [row[6]])\n",
    "\n",
    "        df = pd.read_csv('./mimic_database/PRESCRIPTIONS_reduced_byday.csv')\n",
    "        df['CHARTDAY'] = df['CHARTDAY'].str.split(' ').apply(lambda x: x[0])\n",
    "        df['HADMID_DAY'] = df['HADM_ID'].astype('str') + '_' + df['CHARTDAY']\n",
    "        df['VALUE'] = 1        \n",
    "\n",
    "        cols = ['HADMID_DAY', 'DRUG_FEATURE', 'VALUE']\n",
    "        df = df[cols] \n",
    " \n",
    "        df_pivot = pd.pivot_table(df, index='HADMID_DAY', columns='DRUG_FEATURE', values='VALUE', fill_value=0, aggfunc=np.amax)\n",
    "        df_pivot.reset_index(inplace=True)\n",
    "\n",
    "        df_merged = pd.merge(df_file, df_pivot, on='HADMID_DAY', how='outer')\n",
    " \n",
    "        del df_merged['HADM_ID']\n",
    "        df_merged['HADM_ID'] = df_merged['HADMID_DAY'].str.split('_').apply(lambda x: x[0])\n",
    "        df_merged.fillna(0, inplace=True)\n",
    "\n",
    "        df_merged['dextrose'] = df_merged['dextrose'] + df_merged['D5W']\n",
    "        del df_merged['D5W']\n",
    "\n",
    "        df_merged.to_csv(file_name[0:-4] + '_plus_scripts.csv', index=False)\n",
    "\n",
    "    def add_icd_infect(self, file_name):\n",
    "\n",
    "        df_icd = pd.read_csv('./mimic_database/PROCEDURES_ICD.csv')\n",
    "        df_micro = pd.read_csv('./mimic_database/MICROBIOLOGYEVENTS.csv')\n",
    "        self.suspect_hadmid = set(pd.unique(df_micro['HADM_ID']).tolist())\n",
    "        df_icd_ckd = df_icd[df_icd['ICD9_CODE'] == 585]\n",
    "        \n",
    "        self.ckd = set(df_icd_ckd['HADM_ID'].values.tolist())\n",
    "        \n",
    "        df = pd.read_csv(file_name)\n",
    "        df['CKD'] = df['HADM_ID'].apply(lambda x: 1 if x in self.ckd else 0)\n",
    "        df['Infection'] = df['HADM_ID'].apply(lambda x: 1 if x in self.suspect_hadmid else 0)\n",
    "        df.to_csv(file_name[0:-4] + '_plus_icds.csv', index=False)\n",
    "\n",
    "    def add_notes(self, file_name):\n",
    "        df = pd.read_csv('./mimic_database/NOTEEVENTS.csv')\n",
    "        df_rad_notes = df[['TEXT', 'HADM_ID']][df['CATEGORY'] == 'Radiology']\n",
    "        CTA_bool_array = df_rad_notes['TEXT'].str.contains('CTA', flags=re.IGNORECASE)\n",
    "        CT_angiogram_bool_array = df_rad_notes['TEXT'].str.contains('CT angiogram', flags=re.IGNORECASE)\n",
    "        chest_angiogram_bool_array = df_rad_notes['TEXT'].str.contains('chest angiogram', flags=re.IGNORECASE)\n",
    "        cta_hadm_ids = np.unique(df_rad_notes['HADM_ID'][CTA_bool_array].dropna())\n",
    "        CT_angiogram_hadm_ids = np.unique(df_rad_notes['HADM_ID'][CT_angiogram_bool_array].dropna())\n",
    "        chest_angiogram_hadm_ids = np.unique(df_rad_notes['HADM_ID'][chest_angiogram_bool_array].dropna())\n",
    "        hadm_id_set = set(cta_hadm_ids.tolist())\n",
    "        hadm_id_set.update(CT_angiogram_hadm_ids)\n",
    "        print(len(hadm_id_set))\n",
    "        hadm_id_set.update(chest_angiogram_hadm_ids)\n",
    "        print(len(hadm_id_set))\n",
    "        \n",
    "        df2 = pd.read_csv(file_name)\n",
    "        df2['ct_angio'] = df2['HADM_ID'].apply(lambda x: 1 if x in hadm_id_set else 0)\n",
    "        df2.to_csv(file_name[0:-4] + '_plus_notes.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e606856",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (8,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (8,10,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "blood culture\n",
      "diabetes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp/ipykernel_7264/464692498.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_shard['BLACK'][black_condition] = 1\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp/ipykernel_7264/464692498.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pid.prescriptions['DRUG_FEATURE'][condition] = feature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDAY', 'DRUG_FEATURE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp/ipykernel_7264/567892457.py:17: DtypeWarning: Columns (150,152) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  mp.add_icd_infect(ROOT + FOLDER + FILE_STR + '_24_hour_blocks_plus_admissions_plus_patients_plus_scripts.csv')\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp/ipykernel_7264/567892457.py:18: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  mp.add_notes(ROOT + FOLDER + FILE_STR + '_24_hour_blocks_plus_admissions_plus_patients_plus_scripts_plus_icds.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33909\n",
      "33909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp/ipykernel_7264/567892457.py:18: DtypeWarning: Columns (150,152) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  mp.add_notes(ROOT + FOLDER + FILE_STR + '_24_hour_blocks_plus_admissions_plus_patients_plus_scripts_plus_icds.csv')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    pid = ParseItemID()\n",
    "    pid.build_dictionary()\n",
    "    FOLDER = 'mapped_elements/'\n",
    "    FILE_STR = 'CHARTEVENTS_reduced'\n",
    "    mp = MimicParser()\n",
    "\n",
    "    mp.reduce_total(ROOT + 'CHARTEVENTS.csv')\n",
    "    mp.create_day_blocks(ROOT+ FOLDER + FILE_STR + '.csv')\n",
    "    mp.add_admissions_columns(ROOT + FOLDER + FILE_STR + '_24_hour_blocks.csv')\n",
    "    mp.add_patient_columns(ROOT + FOLDER + FILE_STR + '_24_hour_blocks_plus_admissions.csv')\n",
    "    mp.clean_prescriptions(ROOT + FOLDER + FILE_STR + \n",
    "                         '_24_hour_blocks_plus_admissions_plus_patients.csv')\n",
    "    mp.add_prescriptions(ROOT + FOLDER + FILE_STR + \n",
    "                         '_24_hour_blocks_plus_admissions_plus_patients.csv')\n",
    "    mp.add_icd_infect(ROOT + FOLDER + FILE_STR + '_24_hour_blocks_plus_admissions_plus_patients_plus_scripts.csv') \n",
    "    mp.add_notes(ROOT + FOLDER + FILE_STR + '_24_hour_blocks_plus_admissions_plus_patients_plus_scripts_plus_icds.csv')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea73ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53462ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
